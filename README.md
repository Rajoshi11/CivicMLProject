# IMDb Sentiment Classification via Knowledge Distillation

This repository implements a twoâ€‘stage pipeline:

1. **Teacher**: fineâ€‘tune **BERTâ€‘base** on the IMDb sentiment dataset  
2. **Student**: distil the teacher into **DistilBERT** with oneâ€‘epoch knowledgeâ€‘distillation  

Despite only a single epoch each, we achieve:

| Metric                | Teacher (BERTâ€‘base) | Student (DistilBERT) | StudentÂ /Â TeacherÂ (%) |
|-----------------------|---------------------|----------------------|-----------------------|
| **Accuracy**          | 0.9211              | 0.9111               | 98.9Â %                |
| **F1 Score**          | 0.9210              | 0.9106               | 98.9Â %                |
| **Parameters**        | 109Â 483Â 778         | 66Â 955Â 010           | 61.2Â % (â€“38.8Â %)      |
| **Efficiency Ratio**  | â€”                   | â€”                    | 161.7Â %               |

<p align="center">
  <img src="output.png" alt="Performance dashboard" width="800"/>
</p>

---

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ RujutaCivicML.ipynb            â† solution notebook
â”œâ”€â”€ knowledge_distillation_performance.png
â”œâ”€â”€ loop.mp4
â””â”€â”€ README.md                      â† this file
```

# Run the notebook
Open RujutaCivicML_clean.ipynb in Jupyter or Colab.

Select RunÂ â–¶Â Run All Cells. (Restart after running the first cell)

The notebook will: 

1.Â Bootstrap the environment (pinÂ NumPy, HF libs).

2.Â Fineâ€‘tune BERTâ€‘base for 1Â epoch.

3.Â Cache teacher logits (~1Â min).

4.Â Distil into DistilBERT for 1Â epoch.

5.Â Evaluate both models and render the threeâ€‘panel dashboard.

Total runtime on a freeâ€‘tier T4 GPU: â‰ˆÂ 25Â minutes.

# requirements.txt (Optional)
```
numpy==1.26.4
transformers==4.41.2
datasets>=2.19.0
accelerate
evaluate
scikit-learn
matplotlib
pandas
```

# Results & Interpretation

Accuracy retention: Student keeps ~98.9Â % of teacher accuracy.

Parameter reduction: Student is ~38.8Â % smaller (66Â 955Â 010 vsÂ 109Â 483Â 778).

Efficiency ratio: (accuracy retention)â€‰/â€‰(size fraction) â‰ˆÂ 161.7Â %.

Latency: Student is ~1.9Ã— faster per batch (measured on T4,Â 128â€sample batches).

This demonstrates that knowledge distillation yields a compact, fast model with minimal performance loss.

# Implemented by Rujuta for the Civic ML SummerÂ 2025 Internship assignment.

